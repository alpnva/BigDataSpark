{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a962eb-fb2a-4fee-9fcf-6e14abe9ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, year, month, dayofmonth, quarter, row_number, desc, first, corr, count, lit, concat_ws, avg\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ba0a0",
   "metadata": {},
   "source": [
    "# Работа с ETL процессами\n",
    "\n",
    "## 1. Подключение с помощью PySpark и проверка данных из `mock_data`\n",
    "\n",
    "Нужно преобразовать данные из Postgres `mock_data` в `снежинку`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c11ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/spark-class: line 71: /usr/lib/jvm/temurin-11-jdk-amd64/bin/java: No such file or directory\n",
      "/opt/spark/bin/spark-class: line 97: CMD: bad array subscript\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Инициализация SparkSession с драйвером PostgreSQL\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark://spark-master:7077\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mETL to Star\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Чтение данных из PostgreSQL\u001b[39;00m\n\u001b[1;32m      8\u001b[0m pg_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://postgres:5432/bober_db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# Инициализация SparkSession с драйвером PostgreSQL\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"ETL to Star\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Чтение данных из PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/bober_db\"\n",
    "pg_properties = {\"user\": \"bober\", \"password\": \"bober\", \"driver\": \"org.postgresql.Driver\"}\n",
    "df = spark.read.jdbc(url=pg_url, table=\"mock_data\", properties=pg_properties)\n",
    "\n",
    "# Проверка чтения данных\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54206a",
   "metadata": {},
   "source": [
    "## 2. Создаем модель данных снежинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. dim_date (surrogate key — date_id)\n",
    "# ===================================================================\n",
    "dim_date = df.select(col(\"sale_date\").alias(\"full_date\")) \\\n",
    "    .distinct() \\\n",
    "    .filter(col(\"full_date\").isNotNull()) \\\n",
    "    .withColumn(\"date_id\", row_number().over(Window.orderBy(\"full_date\"))) \\\n",
    "    .withColumn(\"year\", year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"full_date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"full_date\"))\n",
    "\n",
    "dim_date.write.jdbc(url=pg_url, table=\"dim_date\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. dim_customer (natural key — sale_customer_id, предполагаем уникальность)\n",
    "# ===================================================================\n",
    "dim_customer = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_first_name\").alias(\"first_name\"),\n",
    "    col(\"customer_last_name\").alias(\"last_name\"),\n",
    "    col(\"customer_age\").alias(\"age\"),\n",
    "    col(\"customer_email\").alias(\"email\"),\n",
    "    col(\"customer_country\").alias(\"country\"),\n",
    "    col(\"customer_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_customer.write.jdbc(url=pg_url, table=\"dim_customer\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. dim_seller (natural key — sale_seller_id)\n",
    "# ===================================================================\n",
    "dim_seller = df.select(\n",
    "    col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"seller_first_name\").alias(\"first_name\"),\n",
    "    col(\"seller_last_name\").alias(\"last_name\"),\n",
    "    col(\"seller_email\").alias(\"email\"),\n",
    "    col(\"seller_country\").alias(\"country\"),\n",
    "    col(\"seller_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_seller.write.jdbc(url=pg_url, table=\"dim_seller\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. dim_product (natural key — sale_product_id)\n",
    "# ===================================================================\n",
    "dim_product = df.select(\n",
    "    col(\"sale_product_id\").alias(\"product_id\"),\n",
    "    col(\"product_name\").alias(\"name\"),\n",
    "    col(\"product_category\").alias(\"category\"),          # или pet_category — выбирай то, что подходит\n",
    "    col(\"product_price\").alias(\"price\"),\n",
    "    col(\"product_weight\").alias(\"weight\"),\n",
    "    col(\"product_color\").alias(\"color\"),\n",
    "    col(\"product_size\").alias(\"size\"),\n",
    "    col(\"product_brand\").alias(\"brand\"),\n",
    "    col(\"product_material\").alias(\"material\"),\n",
    "    col(\"product_description\").alias(\"description\"),\n",
    "    col(\"product_rating\").alias(\"rating\"),\n",
    "    col(\"product_reviews\").alias(\"reviews\"),\n",
    "    col(\"product_release_date\").alias(\"release_date\"),\n",
    "    col(\"product_expiry_date\").alias(\"expiry_date\")\n",
    ").distinct()\n",
    "\n",
    "dim_product.write.jdbc(url=pg_url, table=\"dim_product\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 5. dim_store (surrogate key)\n",
    "# ===================================================================\n",
    "dim_store_raw = df.select(\n",
    "    \"store_name\", \"store_location\", \"store_city\",\n",
    "    \"store_state\", \"store_country\", \"store_phone\", \"store_email\"\n",
    ").distinct()\n",
    "\n",
    "store_window = Window.orderBy(\"store_name\", \"store_city\", \"store_country\")\n",
    "dim_store = dim_store_raw.withColumn(\"store_id\", row_number().over(store_window))\n",
    "dim_store.write.jdbc(url=pg_url, table=\"dim_store\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 6. dim_supplier (surrogate key)\n",
    "# ===================================================================\n",
    "supplier_window = Window.orderBy(\"supplier_name\", \"supplier_city\", \"supplier_country\")\n",
    "dim_supplier = df.select(\n",
    "    \"supplier_name\",\n",
    "    col(\"supplier_contact\").alias(\"contact\"),\n",
    "    \"supplier_email\",\n",
    "    \"supplier_phone\",\n",
    "    \"supplier_address\",\n",
    "    \"supplier_city\",\n",
    "    \"supplier_country\"\n",
    ").distinct() \\\n",
    "    .withColumn(\"supplier_id\", row_number().over(supplier_window))\n",
    "\n",
    "dim_supplier.write.jdbc(url=pg_url, table=\"dim_supplier\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 7. dim_pet (surrogate key, привязка к клиенту)\n",
    "# ===================================================================\n",
    "dim_pet_raw = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_pet_type\").alias(\"pet_type\"),\n",
    "    col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "    col(\"customer_pet_breed\").alias(\"pet_breed\"),\n",
    "    col(\"pet_category\").alias(\"pet_category\")        # или просто \"category\"\n",
    ").distinct()\n",
    "\n",
    "# Окно определяем ПОСЛЕ select + alias, чтобы использовать новые имена колонок\n",
    "pet_window = Window.orderBy(\"customer_id\", \"pet_name\", \"pet_type\")\n",
    "dim_pet = dim_pet_raw.withColumn(\"pet_id\", row_number().over(pet_window))\n",
    "dim_pet.write.jdbc(url=pg_url, table=\"dim_pet\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 8. fact_sales — собираем всё вместе\n",
    "# ===================================================================\n",
    "fact_sales = df \\\n",
    "    .join(dim_date, df.sale_date == dim_date.full_date, \"left\") \\\n",
    "    .join(dim_store, \n",
    "          (df.store_name == dim_store.store_name) &\n",
    "          (df.store_location == dim_store.store_location) &\n",
    "          (df.store_city == dim_store.store_city) &\n",
    "          (df.store_state == dim_store.store_state) &\n",
    "          (df.store_country == dim_store.store_country) &\n",
    "          (df.store_phone == dim_store.store_phone) &\n",
    "          (df.store_email == dim_store.store_email), \"left\") \\\n",
    "    .join(dim_supplier,\n",
    "          (df.supplier_name == dim_supplier.supplier_name) &\n",
    "          (df.supplier_city == dim_supplier.supplier_city) &\n",
    "          (df.supplier_country == dim_supplier.supplier_country), \"left\") \\\n",
    "    .join(dim_pet,\n",
    "          (df.sale_customer_id == dim_pet.customer_id) &\n",
    "          (df.customer_pet_name == dim_pet.pet_name) &\n",
    "          (df.customer_pet_type == dim_pet.pet_type), \"left\") \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"sale_id\"),\n",
    "        col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "        col(\"pet_id\"),\n",
    "        col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "        col(\"sale_product_id\").alias(\"product_id\"),\n",
    "        col(\"store_id\"),\n",
    "        col(\"supplier_id\"),\n",
    "        col(\"date_id\"),\n",
    "        col(\"sale_quantity\").alias(\"sale_quantity\"),\n",
    "        col(\"sale_total_price\").alias(\"sale_total_price\")\n",
    "    )\n",
    "\n",
    "fact_sales.write.jdbc(url=pg_url, table=\"fact_sales\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "print(\"Звёздная схема успешно построена! Проверить можно в DBeaver: SELECT * FROM fact_sales LIMIT 5;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefaabe",
   "metadata": {},
   "source": [
    "## 3. Создание витрин в clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e82cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 19:24:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Инициализация SparkSession с драйвером ClickHouse\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .appName(\"ETL to Star\") \\\n",
    "#     .config(\"spark.jars\", \"/opt/spark/jars/clickhouse-jdbc-0.6.0.jar\") \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .appName(\"Spark_and_ClickHouse\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClickHouse ETL\") \\\n",
    "    .config(\"spark.jars\",\n",
    "            \"/opt/spark/jars/clickhouse-jdbc-0.6.0.jar,\"\n",
    "            \"/opt/spark/jars/clickhouse-spark-connector_2.12-0.8.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Настройки подключения к БД и Spark\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_options = {\n",
    "    \"host\": \"clickhouse\",\n",
    "    \"port\": \"8123\",\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\",\n",
    "    \"database\": \"default\"\n",
    "}\n",
    "# ch_properties = {\n",
    "#     \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "#     \"user\": \"default\",\n",
    "#     \"password\": \"\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95553d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем все таблицы звезды\n",
    "fact = spark.read.jdbc(url=pg_url, table=\"fact_sales\", properties=pg_properties)\n",
    "dim_product = spark.read.jdbc(url=pg_url, table=\"dim_product\", properties=pg_properties)\n",
    "dim_customer = spark.read.jdbc(url=pg_url, table=\"dim_customer\", properties=pg_properties)\n",
    "dim_store = spark.read.jdbc(url=pg_url, table=\"dim_store\", properties=pg_properties)\n",
    "dim_supplier = spark.read.jdbc(url=pg_url, table=\"dim_supplier\", properties=pg_properties)\n",
    "dim_date = spark.read.jdbc(url=pg_url, table=\"dim_date\", properties=pg_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===================================================================\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "product_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 самых продаваемых (отдельная таблица для удобства проверки)\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "top10_products.write.jdbc(url=ch_url, table=\"top10_sold_products\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Выручка по категориям (отдельная таблица)\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(_sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "category_revenue.write.jdbc(url=ch_url, table=\"category_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===================================================================\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "customer_vitrina.write.jdbc(url=ch_url, table=\"vitrina_customer_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 клиентов\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "top10_customers.write.jdbc(url=ch_url, table=\"top10_customers_by_spent\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Распределение по странам (отдельная таблица)\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "customer_country_dist.write.jdbc(url=ch_url, table=\"customer_country_distribution\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===================================================================\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "time_vitrina.write.jdbc(url=ch_url, table=\"vitrina_time_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===================================================================\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "store_vitrina.write.jdbc(url=ch_url, table=\"vitrina_store_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 магазинов\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_stores.write.jdbc(url=ch_url, table=\"top5_stores_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===================================================================\n",
    "supplier_vitrina = fact.join(dim_product[[\"product_id\", \"price\"]], fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "supplier_vitrina.write.jdbc(url=ch_url, table=\"vitrina_supplier_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 поставщиков\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_suppliers.write.jdbc(url=ch_url, table=\"top5_suppliers_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 6. Витрина качества продукции\n",
    "# ===================================================================\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "quality_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_quality\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Корреляция (одна строка — отдельная таблица)\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "correlation.write.jdbc(url=ch_url, table=\"product_quality_correlation\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "print(\"Все 6 витрин + топы + корреляция успешно загружены в ClickHouse!\")\n",
    "print(\"Проверить можно в DBeaver или clickhouse-client:\")\n",
    "print(\"SELECT * FROM vitrina_product_sales LIMIT 10;\")\n",
    "print(\"SELECT corr(rating, total_quantity) FROM vitrina_product_quality;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab2290",
   "metadata": {},
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acc2b76-ce20-4548-80b0-4add4ed00ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e14cf9e-ea44-43ac-ac23-f0103934d8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:java.lang.Class.forName.\n: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Unknown Source)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclickhouse.DefaultSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:java.lang.Class.forName.\n: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Unknown Source)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "spark._jvm.Class.forName(\"clickhouse.DefaultSource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1cf077",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: clickhouse. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Витрина продаж по продуктам\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m product_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_product, fact\u001b[38;5;241m.\u001b[39mproduct_id \u001b[38;5;241m==\u001b[39m dim_product\u001b[38;5;241m.\u001b[39mproduct_id) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_product\u001b[38;5;241m.\u001b[39mproduct_id, dim_product\u001b[38;5;241m.\u001b[39mname, dim_product\u001b[38;5;241m.\u001b[39mcategory) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         first(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mproduct_vitrina\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclickhouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mch_options\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvitrina_product_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreateTableOptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mENGINE = MergeTree() ORDER BY product_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Топ-10 самых продаваемых\u001b[39;00m\n\u001b[1;32m     22\u001b[0m top10_products \u001b[38;5;241m=\u001b[39m product_vitrina\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: clickhouse. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===============================\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "\n",
    "product_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_product_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Топ-10 самых продаваемых\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "top10_products.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top10_sold_products\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Выручка по категориям\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(_sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "category_revenue.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"category_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY category\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===============================\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "\n",
    "customer_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_customer_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY customer_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "top10_customers.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top10_customers_by_spent\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY customer_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "customer_country_dist.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"customer_country_distribution\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===============================\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "\n",
    "time_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_time_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY (year, month)\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===============================\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "\n",
    "store_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_store_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_stores.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top5_stores_by_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===============================\n",
    "supplier_vitrina = fact.join(dim_product[[\"product_id\", \"price\"]], fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "\n",
    "supplier_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_supplier_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY supplier_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_suppliers.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top5_suppliers_by_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY supplier_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 6. Витрина качества продукции\n",
    "# ===============================\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "quality_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_product_quality\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Корреляция\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "\n",
    "correlation.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"product_quality_correlation\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY description\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Все 6 витрин + топы + корреляция успешно загружены в ClickHouse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53b82cc-1050-47e2-90b7-abb408d78ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершаем сессию Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
