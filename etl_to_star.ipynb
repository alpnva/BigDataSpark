{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66ba0a0",
   "metadata": {},
   "source": [
    "# Работа с ETL процессами\n",
    "\n",
    "## 1. Подключение с помощью PySpark и проверка данных из `mock_data`\n",
    "\n",
    "Нужно преобразовать данные из Postgres `mock_data` в `снежинку`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c11ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 12:42:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, customer_first_name='Barron', customer_last_name='Rawlyns', customer_age=61, customer_email='bmassingham0@army.mil', customer_country='China', customer_postal_code=None, customer_pet_type='cat', customer_pet_name='Priscella', customer_pet_breed='Labrador Retriever', seller_first_name='Bevan', seller_last_name='Massingham', seller_email='bmassingham0@answers.com', seller_country='Indonesia', seller_postal_code=None, product_name='Dog Food', product_category='Food', product_price=Decimal('77.97'), product_quantity=89, sale_date=datetime.date(2021, 5, 14), sale_customer_id=1, sale_seller_id=1, sale_product_id=1, sale_quantity=4, sale_total_price=Decimal('487.70'), store_name='Youopia', store_location='Suite 75', store_city='Xichehe', store_state=None, store_country='United States', store_phone='564-244-8660', store_email='bmassingham0@networkadvertising.org', pet_category='Cats', product_weight=Decimal('13.40'), product_color='Indigo', product_size='Medium', product_brand='Skajo', product_material='Steel', product_description='Aliquam quis turpis eget elit sodales scelerisque. Mauris sit amet eros. Suspendisse accumsan tortor quis turpis.\\n\\nSed ante. Vivamus tortor. Duis mattis egestas metus.', product_rating=Decimal('2.1'), product_reviews=97, product_release_date=datetime.date(2011, 10, 19), product_expiry_date=datetime.date(2028, 10, 21), supplier_name='Tagcat', supplier_contact='Bevan Massingham', supplier_email='bmassingham0@unblog.fr', supplier_phone='914-877-7062', supplier_address='Suite 25', supplier_city='Kletek', supplier_country='China'),\n",
       " Row(id=2, customer_first_name='Ham', customer_last_name='Knowller', customer_age=78, customer_email='cscudder1@time.com', customer_country='Poland', customer_postal_code='73-115', customer_pet_type='bird', customer_pet_name='Dalenna', customer_pet_breed='Labrador Retriever', seller_first_name='Candide', seller_last_name='Scudder', seller_email='cscudder1@xing.com', seller_country='Guatemala', seller_postal_code='04015', product_name='Cat Toy', product_category='Food', product_price=Decimal('48.70'), product_quantity=73, sale_date=datetime.date(2021, 11, 13), sale_customer_id=2, sale_seller_id=2, sale_product_id=2, sale_quantity=10, sale_total_price=Decimal('484.61'), store_name='Fivespan', store_location='Suite 1', store_city='Dolice', store_state=None, store_country='Lithuania', store_phone='596-788-0321', store_email='cscudder1@behance.net', pet_category='Cats', product_weight=Decimal('36.30'), product_color='Khaki', product_size='Small', product_brand='Wordpedia', product_material='Plexiglass', product_description='Phasellus in felis. Donec semper sapien a libero. Nam dui.', product_rating=Decimal('2.4'), product_reviews=37, product_release_date=datetime.date(2019, 4, 17), product_expiry_date=datetime.date(2028, 2, 29), supplier_name='Livetube', supplier_contact='Candide Scudder', supplier_email='cscudder1@sbwire.com', supplier_phone='863-319-5653', supplier_address='18th Floor', supplier_city='Santa Cruz Balanyá', supplier_country='Poland'),\n",
       " Row(id=3, customer_first_name='Farleigh', customer_last_name='Langley', customer_age=71, customer_email='vhuxter2@fotki.com', customer_country='Poland', customer_postal_code='56-160', customer_pet_type='bird', customer_pet_name='Aldridge', customer_pet_breed='Parakeet', seller_first_name='Vinson', seller_last_name='Huxter', seller_email='vhuxter2@issuu.com', seller_country='China', seller_postal_code=None, product_name='Bird Cage', product_category='Food', product_price=Decimal('77.75'), product_quantity=39, sale_date=datetime.date(2021, 12, 4), sale_customer_id=3, sale_seller_id=3, sale_product_id=3, sale_quantity=9, sale_total_price=Decimal('144.24'), store_name='Avamba', store_location='Suite 69', store_city='Wińsko', store_state=None, store_country='South Africa', store_phone='148-671-4697', store_email='vhuxter2@marketwatch.com', pet_category='Fish', product_weight=Decimal('14.60'), product_color='Khaki', product_size='Medium', product_brand='Riffpedia', product_material='Plexiglass', product_description='Maecenas leo odio, condimentum id, luctus nec, molestie sed, justo. Pellentesque viverra pede ac diam. Cras pellentesque volutpat dui.\\n\\nMaecenas tristique, est et tempus semper, est quam pharetra magna, ac consequat metus sapien ut nunc. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Mauris viverra diam vitae quam. Suspendisse potenti.\\n\\nNullam porttitor lacus at turpis. Donec posuere metus vitae ipsum. Aliquam non mauris.', product_rating=Decimal('3.0'), product_reviews=218, product_release_date=datetime.date(2010, 2, 3), product_expiry_date=datetime.date(2023, 9, 3), supplier_name='Photobug', supplier_contact='Vinson Huxter', supplier_email='vhuxter2@slate.com', supplier_phone='434-817-1275', supplier_address='Apt 96', supplier_city='Huangpu', supplier_country='Samoa')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, year, month, dayofmonth, quarter, row_number, desc, first, corr, count, lit, concat_ws, avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Инициализация SparkSession с драйвером PostgreSQL\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"ETL to Star\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.4.jar,/opt/spark/jars/clickhouse-jdbc-0.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Чтение данных из PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/bober_db\"\n",
    "pg_properties = {\"user\": \"bober\", \"password\": \"bober\", \"driver\": \"org.postgresql.Driver\"}\n",
    "df = spark.read.jdbc(url=pg_url, table=\"mock_data\", properties=pg_properties)\n",
    "\n",
    "# Проверка чтения данных\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54206a",
   "metadata": {},
   "source": [
    "## 2. Создаем модель данных снежинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_name == store_name'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_location == store_location'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_city == store_city'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_state == store_state'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_country == store_country'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_phone == store_phone'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'store_email == store_email'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'supplier_name == supplier_name'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'supplier_city == supplier_city'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN Column: Constructing trivially true equals predicate, 'supplier_country == supplier_country'. Perhaps you need to use aliases.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/17 11:38:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Звёздная схема успешно построена! Проверить можно в DBeaver: SELECT * FROM fact_sales LIMIT 5;\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. dim_date (surrogate key — date_id)\n",
    "# ===================================================================\n",
    "dim_date = df.select(col(\"sale_date\").alias(\"full_date\")) \\\n",
    "    .distinct() \\\n",
    "    .filter(col(\"full_date\").isNotNull()) \\\n",
    "    .withColumn(\"date_id\", row_number().over(Window.orderBy(\"full_date\"))) \\\n",
    "    .withColumn(\"year\", year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"full_date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"full_date\"))\n",
    "\n",
    "dim_date.write.jdbc(url=pg_url, table=\"dim_date\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. dim_customer (natural key — sale_customer_id, предполагаем уникальность)\n",
    "# ===================================================================\n",
    "dim_customer = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_first_name\").alias(\"first_name\"),\n",
    "    col(\"customer_last_name\").alias(\"last_name\"),\n",
    "    col(\"customer_age\").alias(\"age\"),\n",
    "    col(\"customer_email\").alias(\"email\"),\n",
    "    col(\"customer_country\").alias(\"country\"),\n",
    "    col(\"customer_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_customer.write.jdbc(url=pg_url, table=\"dim_customer\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. dim_seller (natural key — sale_seller_id)\n",
    "# ===================================================================\n",
    "dim_seller = df.select(\n",
    "    col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"seller_first_name\").alias(\"first_name\"),\n",
    "    col(\"seller_last_name\").alias(\"last_name\"),\n",
    "    col(\"seller_email\").alias(\"email\"),\n",
    "    col(\"seller_country\").alias(\"country\"),\n",
    "    col(\"seller_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_seller.write.jdbc(url=pg_url, table=\"dim_seller\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. dim_product (natural key — sale_product_id)\n",
    "# ===================================================================\n",
    "dim_product = df.select(\n",
    "    col(\"sale_product_id\").alias(\"product_id\"),\n",
    "    col(\"product_name\").alias(\"name\"),\n",
    "    col(\"product_category\").alias(\"category\"),          # или pet_category — выбирай то, что подходит\n",
    "    col(\"product_price\").alias(\"price\"),\n",
    "    col(\"product_weight\").alias(\"weight\"),\n",
    "    col(\"product_color\").alias(\"color\"),\n",
    "    col(\"product_size\").alias(\"size\"),\n",
    "    col(\"product_brand\").alias(\"brand\"),\n",
    "    col(\"product_material\").alias(\"material\"),\n",
    "    col(\"product_description\").alias(\"description\"),\n",
    "    col(\"product_rating\").alias(\"rating\"),\n",
    "    col(\"product_reviews\").alias(\"reviews\"),\n",
    "    col(\"product_release_date\").alias(\"release_date\"),\n",
    "    col(\"product_expiry_date\").alias(\"expiry_date\")\n",
    ").distinct()\n",
    "\n",
    "dim_product.write.jdbc(url=pg_url, table=\"dim_product\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 5. dim_store (surrogate key)\n",
    "# ===================================================================\n",
    "dim_store_raw = df.select(\n",
    "    \"store_name\", \"store_location\", \"store_city\",\n",
    "    \"store_state\", \"store_country\", \"store_phone\", \"store_email\"\n",
    ").distinct()\n",
    "\n",
    "store_window = Window.orderBy(\"store_name\", \"store_city\", \"store_country\")\n",
    "dim_store = dim_store_raw.withColumn(\"store_id\", row_number().over(store_window))\n",
    "dim_store.write.jdbc(url=pg_url, table=\"dim_store\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 6. dim_supplier (surrogate key)\n",
    "# ===================================================================\n",
    "supplier_window = Window.orderBy(\"supplier_name\", \"supplier_city\", \"supplier_country\")\n",
    "dim_supplier = df.select(\n",
    "    \"supplier_name\",\n",
    "    col(\"supplier_contact\").alias(\"contact\"),\n",
    "    \"supplier_email\",\n",
    "    \"supplier_phone\",\n",
    "    \"supplier_address\",\n",
    "    \"supplier_city\",\n",
    "    \"supplier_country\"\n",
    ").distinct() \\\n",
    "    .withColumn(\"supplier_id\", row_number().over(supplier_window))\n",
    "\n",
    "dim_supplier.write.jdbc(url=pg_url, table=\"dim_supplier\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 7. dim_pet (surrogate key, привязка к клиенту)\n",
    "# ===================================================================\n",
    "dim_pet_raw = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_pet_type\").alias(\"pet_type\"),\n",
    "    col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "    col(\"customer_pet_breed\").alias(\"pet_breed\"),\n",
    "    col(\"pet_category\").alias(\"pet_category\")        # или просто \"category\"\n",
    ").distinct()\n",
    "\n",
    "# Окно определяем ПОСЛЕ select + alias, чтобы использовать новые имена колонок\n",
    "pet_window = Window.orderBy(\"customer_id\", \"pet_name\", \"pet_type\")\n",
    "dim_pet = dim_pet_raw.withColumn(\"pet_id\", row_number().over(pet_window))\n",
    "dim_pet.write.jdbc(url=pg_url, table=\"dim_pet\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 8. fact_sales — собираем всё вместе\n",
    "# ===================================================================\n",
    "fact_sales = df \\\n",
    "    .join(dim_date, df.sale_date == dim_date.full_date, \"left\") \\\n",
    "    .join(dim_store, \n",
    "          (df.store_name == dim_store.store_name) &\n",
    "          (df.store_location == dim_store.store_location) &\n",
    "          (df.store_city == dim_store.store_city) &\n",
    "          (df.store_state == dim_store.store_state) &\n",
    "          (df.store_country == dim_store.store_country) &\n",
    "          (df.store_phone == dim_store.store_phone) &\n",
    "          (df.store_email == dim_store.store_email), \"left\") \\\n",
    "    .join(dim_supplier,\n",
    "          (df.supplier_name == dim_supplier.supplier_name) &\n",
    "          (df.supplier_city == dim_supplier.supplier_city) &\n",
    "          (df.supplier_country == dim_supplier.supplier_country), \"left\") \\\n",
    "    .join(dim_pet,\n",
    "          (df.sale_customer_id == dim_pet.customer_id) &\n",
    "          (df.customer_pet_name == dim_pet.pet_name) &\n",
    "          (df.customer_pet_type == dim_pet.pet_type), \"left\") \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"sale_id\"),\n",
    "        col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "        col(\"pet_id\"),\n",
    "        col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "        col(\"sale_product_id\").alias(\"product_id\"),\n",
    "        col(\"store_id\"),\n",
    "        col(\"supplier_id\"),\n",
    "        col(\"date_id\"),\n",
    "        col(\"sale_quantity\").alias(\"sale_quantity\"),\n",
    "        col(\"sale_total_price\").alias(\"sale_total_price\")\n",
    "    )\n",
    "\n",
    "fact_sales.write.jdbc(url=pg_url, table=\"fact_sales\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "print(\"Звёздная схема успешно построена! Проверить можно в DBeaver: SELECT * FROM fact_sales LIMIT 5;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefaabe",
   "metadata": {},
   "source": [
    "## 3. Создание витрин в clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e82cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки подключения к БД и Spark\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_properties = {\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95553d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем все таблицы звезды\n",
    "fact = spark.read.jdbc(url=pg_url, table=\"fact_sales\", properties=pg_properties)\n",
    "dim_product = spark.read.jdbc(url=pg_url, table=\"dim_product\", properties=pg_properties)\n",
    "dim_customer = spark.read.jdbc(url=pg_url, table=\"dim_customer\", properties=pg_properties)\n",
    "dim_store = spark.read.jdbc(url=pg_url, table=\"dim_store\", properties=pg_properties)\n",
    "dim_supplier = spark.read.jdbc(url=pg_url, table=\"dim_supplier\", properties=pg_properties)\n",
    "dim_date = spark.read.jdbc(url=pg_url, table=\"dim_date\", properties=pg_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873f17d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1270.jdbc.\n: java.lang.ClassNotFoundException: com.clickhouse.jdbc.ClickHouseDriver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat jdk.internal.reflect.GeneratedMethodAccessor96.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\t\tat scala.Option.foreach(Option.scala:437)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Витрина продаж по продуктам\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m product_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_product, fact\u001b[38;5;241m.\u001b[39mproduct_id \u001b[38;5;241m==\u001b[39m dim_product\u001b[38;5;241m.\u001b[39mproduct_id) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_product\u001b[38;5;241m.\u001b[39mproduct_id, dim_product\u001b[38;5;241m.\u001b[39mname, dim_product\u001b[38;5;241m.\u001b[39mcategory) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         first(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mproduct_vitrina\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mch_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvitrina_product_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mch_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Топ-10 самых продаваемых (отдельная таблица для удобства проверки)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m top10_products \u001b[38;5;241m=\u001b[39m product_vitrina\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:2347\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   2346\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 2347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1270.jdbc.\n: java.lang.ClassNotFoundException: com.clickhouse.jdbc.ClickHouseDriver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat jdk.internal.reflect.GeneratedMethodAccessor96.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\t\tat scala.Option.foreach(Option.scala:437)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===================================================================\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "product_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 самых продаваемых (отдельная таблица для удобства проверки)\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "top10_products.write.jdbc(url=ch_url, table=\"top10_sold_products\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Выручка по категориям (отдельная таблица)\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(_sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "category_revenue.write.jdbc(url=ch_url, table=\"category_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===================================================================\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "customer_vitrina.write.jdbc(url=ch_url, table=\"vitrina_customer_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 клиентов\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "top10_customers.write.jdbc(url=ch_url, table=\"top10_customers_by_spent\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Распределение по странам (отдельная таблица)\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "customer_country_dist.write.jdbc(url=ch_url, table=\"customer_country_distribution\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===================================================================\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "time_vitrina.write.jdbc(url=ch_url, table=\"vitrina_time_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===================================================================\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "store_vitrina.write.jdbc(url=ch_url, table=\"vitrina_store_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 магазинов\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_stores.write.jdbc(url=ch_url, table=\"top5_stores_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===================================================================\n",
    "supplier_vitrina = fact.join(dim_product[[\"product_id\", \"price\"]], fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "supplier_vitrina.write.jdbc(url=ch_url, table=\"vitrina_supplier_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 поставщиков\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_suppliers.write.jdbc(url=ch_url, table=\"top5_suppliers_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 6. Витрина качества продукции\n",
    "# ===================================================================\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "quality_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_quality\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Корреляция (одна строка — отдельная таблица)\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "correlation.write.jdbc(url=ch_url, table=\"product_quality_correlation\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "print(\"Все 6 витрин + топы + корреляция успешно загружены в ClickHouse!\")\n",
    "print(\"Проверить можно в DBeaver или clickhouse-client:\")\n",
    "print(\"SELECT * FROM vitrina_product_sales LIMIT 10;\")\n",
    "print(\"SELECT corr(rating, total_quantity) FROM vitrina_product_quality;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab2290",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53b82cc-1050-47e2-90b7-abb408d78ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершаем сессию Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
